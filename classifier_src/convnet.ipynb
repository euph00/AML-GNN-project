{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b190d5",
   "metadata": {},
   "source": [
    "# Simple Graph Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b260530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from modules.data_loader import *\n",
    "from modules.feature_engineering import *\n",
    "from modules.visualizer import *\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2913730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run if you dont have the data downloaded this is a few gb of data\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3353db",
   "metadata": {},
   "source": [
    "## 1. Create our dataframes from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4923a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HI-Small...\n",
      "\n",
      "\n",
      "Loading transactions from: /home/linch/.cache/kagglehub/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/versions/8/HI-Small_Trans.csv\n",
      "File size: 453.6 MB\n",
      "\n",
      "Loaded 5,078,345 transactions\n",
      "Date range: 2022-09-01 00:00:00 to 2022-09-18 16:18:00\n",
      "Laundering transactions: 5,177 (0.102%)\n",
      "   transaction_id           timestamp from_bank from_account to_bank  \\\n",
      "0               0 2022-09-01 00:20:00       010    8000EBD30     010   \n",
      "1               1 2022-09-01 00:20:00     03208    8000F4580     001   \n",
      "2               2 2022-09-01 00:00:00     03209    8000F4670   03209   \n",
      "3               3 2022-09-01 00:02:00       012    8000F5030     012   \n",
      "4               4 2022-09-01 00:06:00       010    8000F5200     010   \n",
      "\n",
      "  to_account  amount_received receiving_currency  amount_paid  \\\n",
      "0  8000EBD30          3697.34          US Dollar      3697.34   \n",
      "1  8000F5340             0.01          US Dollar         0.01   \n",
      "2  8000F4670         14675.57          US Dollar     14675.57   \n",
      "3  8000F5030          2806.97          US Dollar      2806.97   \n",
      "4  8000F5200         36682.97          US Dollar     36682.97   \n",
      "\n",
      "  payment_currency payment_format  is_laundering  \n",
      "0        US Dollar   Reinvestment              0  \n",
      "1        US Dollar         Cheque              0  \n",
      "2        US Dollar   Reinvestment              0  \n",
      "3        US Dollar   Reinvestment              0  \n",
      "4        US Dollar   Reinvestment              0  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"HI-Small\"\n",
    "\n",
    "print(f\"Loading {dataset_name}...\\n\")\n",
    "trans_df = load_transactions(dataset_size=dataset_name)\n",
    "# accounts_df = load_accounts(dataset_size=dataset_name)\n",
    "# patterns_df = load_patterns(dataset_size=dataset_name)\n",
    "print(trans_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428fcaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering']\n"
     ]
    }
   ],
   "source": [
    "# convert all currencies to USD for normaliztion\n",
    "\n",
    "trans_df = convert_currency_to_USD(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabcfde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized']\n"
     ]
    }
   ],
   "source": [
    "# compute sinusoidal temporal encodings and normalized unix timestamp\n",
    "\n",
    "trans_df = temporal_encoding(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fcd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id']\n"
     ]
    }
   ],
   "source": [
    "# give each currency and payment method a unique integer ID\n",
    "\n",
    "trans_df = encode_currency_ids(trans_df)\n",
    "trans_df = encode_payment_format_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ac1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n"
     ]
    }
   ],
   "source": [
    "# give each account a unique integer ID\n",
    "\n",
    "trans_df, account_to_id, id_to_account = encode_account_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd99d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = normalize_amounts(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6kt67xtz4c9",
   "metadata": {},
   "source": [
    "## 2. Temporal train/test split and account statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0nzrvv4m8bqr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL TRAIN/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "Train Set:\n",
      "  Date range: 2022-09-01 00:00:00 to 2022-09-08 16:12:00\n",
      "  Transactions: 4,062,676\n",
      "  Laundering: 3,380 (0.083%)\n",
      "\n",
      "Test Set:\n",
      "  Date range: 2022-09-08 16:12:00 to 2022-09-18 16:18:00\n",
      "  Transactions: 1,015,669\n",
      "  Laundering: 1,797 (0.177%)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data temporally: first 80% for training, last 20% for testing\n",
    "\n",
    "train_df, test_df = temporal_train_test_split(trans_df, train_ratio=0.8)\n",
    "\n",
    "import gc\n",
    "del trans_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a427ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n",
      "   transaction_id  timestamp from_bank from_account to_bank to_account  \\\n",
      "0          316720 2022-09-01      0121    8123FB9B0    0121  8123FB9B0   \n",
      "\n",
      "   amount_received receiving_currency  amount_paid payment_currency  \\\n",
      "0        -1.529313        Saudi Riyal    -1.529313      Saudi Riyal   \n",
      "\n",
      "  payment_format  is_laundering  hour_sin  hour_cos  time_normalized  \\\n",
      "0   Reinvestment              0       0.0       1.0         -1.37763   \n",
      "\n",
      "  payment_currency_id receiving_currency_id payment_format_id  \\\n",
      "0                   8                     8                 5   \n",
      "\n",
      "   from_account_id  to_account_id  \n",
      "0           458857         458857  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.tolist())\n",
    "print(train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb74eed",
   "metadata": {},
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_df(df):\n",
    "    source_accounts = df['from_account_id'].values\n",
    "    dest_accounts = df['to_account_id'].values\n",
    "\n",
    "    graph = dgl.graph((source_accounts, dest_accounts))\n",
    "    print(f\"Number of nodes: {graph.num_nodes()}\")\n",
    "    print(f\"Number of edges: {graph.num_edges()}\")\n",
    "    # EDGE FEATS\n",
    "\n",
    "    # bookkeeping\n",
    "    graph.edata['transaction_id'] = torch.tensor(df['transaction_id'].values, dtype=torch.long)\n",
    "    graph.edata['is_laundering'] = torch.tensor(df['is_laundering'].values, dtype=torch.long) #not used in training\n",
    "\n",
    "    # numericals: we can include them directly\n",
    "    numericals = ['amount_received', 'amount_paid', 'hour_sin', 'hour_cos', 'time_normalized']\n",
    "    edge_numerical = torch.tensor(df[numericals].values, dtype=torch.float32)\n",
    "    graph.edata['numericals'] = edge_numerical\n",
    "\n",
    "    # categoricals: we use learned embeddings. This makes them transductive, but that is ok because currencies and payment types don't change often\n",
    "\n",
    "    payment_currency = torch.tensor(df['payment_currency_id'].values, dtype=torch.long)\n",
    "    receiving_currency = torch.tensor(df['receiving_currency_id'].values, dtype=torch.long)\n",
    "    payment_format = torch.tensor(df['payment_format_id'].values, dtype=torch.long)\n",
    "    graph.edata['payment_currency'] = payment_currency\n",
    "    graph.edata['receiving_currency'] = receiving_currency\n",
    "    graph.edata['payment_format'] = payment_format\n",
    "\n",
    "    # NODE FEATS\n",
    "\n",
    "    # For each account, we compute their in and outdegrees\n",
    "    # We zero-center and log transform this value and normalize by the graph's average transformed in and outdegrees so that our model is not sensitive to the graph size. Our training graph has 80% of the edges, so naturally the raw degree counts will be higher than the testing graph.\n",
    "    indegrees = graph.in_degrees().float()\n",
    "    outdegrees = graph.out_degrees().float()\n",
    "\n",
    "    log_indegrees = torch.log(indegrees + 1)\n",
    "    log_outdegrees = torch.log(outdegrees + 1)\n",
    "\n",
    "    avg_log_indegree = log_indegrees.mean()\n",
    "    avg_log_outdegree = log_outdegrees.mean()\n",
    "\n",
    "    # zc and normalize\n",
    "    normalized_indegree = (log_indegrees - avg_log_indegree) / avg_log_indegree\n",
    "    normalized_outdegree = (log_outdegrees - avg_log_outdegree) / avg_log_outdegree\n",
    "\n",
    "    node_features = torch.stack([normalized_indegree, normalized_outdegree], dim=1)\n",
    "\n",
    "    # Perhaps we can include simple graph RWPE as a node feature too?\n",
    "    graph.ndata['node_feats'] = node_features\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093e2fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 515080\n",
      "Number of edges: 4062676\n",
      "Number of nodes: 515080\n",
      "Number of edges: 1015669\n"
     ]
    }
   ],
   "source": [
    "training_graph = build_graph_from_df(train_df)\n",
    "test_graph = build_graph_from_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5c3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Node Features (normalized degrees) =====\n",
      "\n",
      "----- Node Degrees (first 10 nodes) -----\n",
      "Node 0:\n",
      "  In-degree:  545  → normalized: +3.1889\n",
      "  Out-degree: 132597 → normalized: +7.6085\n",
      "Node 1:\n",
      "  In-degree:  328  → normalized: +2.8523\n",
      "  Out-degree: 80883 → normalized: +7.2477\n",
      "Node 2:\n",
      "  In-degree:  55  → normalized: +1.6754\n",
      "  Out-degree: 14711 → normalized: +6.0038\n",
      "Node 3:\n",
      "  In-degree:  53  → normalized: +1.6512\n",
      "  Out-degree: 10810 → normalized: +5.7789\n",
      "Node 4:\n",
      "  In-degree:  61  → normalized: +1.7430\n",
      "  Out-degree: 13641 → normalized: +5.9487\n",
      "\n",
      "===== Edge Features =====\n",
      "Numerical features shape: torch.Size([4062676, 5])\n",
      "Categorical feature 1 shape: torch.Size([4062676])\n",
      "Categorical feature 2 shape: torch.Size([4062676])\n",
      "Categorical feature 3 shape: torch.Size([4062676])\n",
      "\n",
      "===== Sample Edge Features =====\n",
      "Edge 0: 458857 -> 458857\n",
      "  Numerical: [-1.5293132066726685, -1.529313087463379, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 8, receiving currency: 8 , payment format: 5\n",
      "Edge 1: 236631 -> 236631\n",
      "  Numerical: [0.3639656603336334, 0.3639656603336334, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 5\n",
      "Edge 2: 244237 -> 229946\n",
      "  Numerical: [-0.9314213991165161, -0.9314213991165161, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 4\n",
      "Edge 3: 20992 -> 20994\n",
      "  Numerical: [0.9639478325843811, 0.9639478325843811, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 6\n",
      "Edge 4: 19729 -> 19729\n",
      "  Numerical: [-1.5437047481536865, -1.5437047481536865, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 5\n"
     ]
    }
   ],
   "source": [
    "# Check graph sanity\n",
    "\n",
    "print(\"\\n===== Node Features (normalized degrees) =====\")\n",
    "print(\"\\n----- Node Degrees (first 10 nodes) -----\")\n",
    "indegrees = training_graph.in_degrees().float()\n",
    "outdegrees = training_graph.out_degrees().float()\n",
    "\n",
    "for i in range(5):\n",
    "    norm_in = training_graph.ndata['node_feats'][i, 0].item()\n",
    "    norm_out = training_graph.ndata['node_feats'][i, 1].item()\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  In-degree:  {indegrees[i].item():.0f}  → normalized: {norm_in:+.4f}\")\n",
    "    print(f\"  Out-degree: {outdegrees[i].item():.0f} → normalized: {norm_out:+.4f}\")\n",
    "\n",
    "print(\"\\n===== Edge Features =====\")\n",
    "print(f\"Numerical features shape: {training_graph.edata['numericals'].shape}\")\n",
    "print(f\"Categorical feature 1 shape: {training_graph.edata['payment_currency'].shape}\")\n",
    "print(f\"Categorical feature 2 shape: {training_graph.edata['receiving_currency'].shape}\")\n",
    "print(f\"Categorical feature 3 shape: {training_graph.edata['payment_format'].shape}\")\n",
    "\n",
    "print(\"\\n===== Sample Edge Features =====\")\n",
    "for i in range(5):\n",
    "    src, dst = training_graph.edges()\n",
    "    print(f\"Edge {i}: {src[i].item()} -> {dst[i].item()}\")\n",
    "    print(f\"  Numerical: {training_graph.edata['numericals'][i].tolist()}\")\n",
    "    print(f\"  payment currency: {training_graph.edata['payment_currency'][i].item()}, receiving currency: {training_graph.edata['receiving_currency'][i].item()} , payment format: {training_graph.edata['payment_format'][i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3610b26",
   "metadata": {},
   "source": [
    "# Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1d378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have both categorical and numerical edge features, we need a class to process and combine them\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_currencies=15, \n",
    "            num_payment_methods=7, \n",
    "            currencies_embed_dim=8, \n",
    "            payment_embed_dim=4, \n",
    "            num_numericals=5):\n",
    "        super().__init__()\n",
    "        self.currencies_embed = nn.Embedding(num_currencies, currencies_embed_dim)\n",
    "        self.payment_embed = nn.Embedding(num_payment_methods, payment_embed_dim)\n",
    "\n",
    "        # each edge has payment currency, receiving currency, payment method and numericals\n",
    "        self.out_dim = currencies_embed_dim + currencies_embed_dim + payment_embed_dim + num_numericals\n",
    "\n",
    "    def forward(self, payment_curr, receiving_curr, payment_method, numericals):\n",
    "        payment_curr_embed = self.currencies_embed(payment_curr)\n",
    "        receiving_curr_embed = self.currencies_embed(receiving_curr)\n",
    "        payment_method_embed = self.payment_embed(payment_method)\n",
    "\n",
    "        edge_feats = torch.cat([payment_curr_embed, receiving_curr_embed, payment_method_embed, numericals], dim=1)\n",
    "        return edge_feats\n",
    "\n",
    "# this returns our GNN-ready edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message passing layers\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, residual=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.residual = residual\n",
    "\n",
    "        if residual and input_dim != output_dim:\n",
    "            self.res_linear = nn.Linear(input_dim, output_dim) # project residual if diff dims\n",
    "        elif residual:\n",
    "            self.res_linear = nn.Identity()\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m': edges.src['Wh'] * edges.src['norm']} # normalize by source degree first\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        return {'h': torch.sum(nodes.mailbox['m'], dim=1)}\n",
    "    \n",
    "    def forward(self, graph, node_feats):\n",
    "        with graph.local_scope():\n",
    "            h_in = node_feats\n",
    "\n",
    "            graph.ndata['Wh'] = self.linear(node_feats) # W is linear transform, h is node feats\n",
    "\n",
    "            degs = graph.in_degrees().float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5).unsqueeze(1)\n",
    "            graph.ndata['norm'] = norm\n",
    "\n",
    "            graph.update_all(self.message_func, self.reduce_func)\n",
    "\n",
    "            h_out = graph.ndata['h'] * norm # then normalize by destination degree\n",
    "\n",
    "            h_out = F.relu(h_out)\n",
    "            if self.residual:\n",
    "                return h_out + self.res_linear(h_in)\n",
    "            else:\n",
    "                return h_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a363413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classifier neural network\n",
    "\n",
    "class GCNEdgeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                # node params\n",
    "                node_in_feats=2, # log transformed and normalized indegree outdegree\n",
    "                hidden_dim=64,\n",
    "                num_gcn_layers=3,\n",
    "\n",
    "                # for EdgeEmbedding\n",
    "                num_currencies=15,\n",
    "                num_payment_methods=7,\n",
    "                currencies_embed_dim=8,\n",
    "                payment_embed_dim=4,\n",
    "                num_numericals=5,\n",
    "\n",
    "                # output classes\n",
    "                num_classes=2,\n",
    "\n",
    "                # regularization\n",
    "                dropout=0.2,\n",
    "                use_batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_gcn_layers = num_gcn_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(node_in_feats, hidden_dim, residual=False)) # no residual for layer 1 since in dim and out dim dont match, simpler to just skip\n",
    "        for i in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(hidden_dim, hidden_dim, residual=True))\n",
    "        if use_batch_norm: # create batch norm layers if using them\n",
    "            self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for i in range(num_gcn_layers)])\n",
    "\n",
    "        # Edge Feature processor\n",
    "        self.edge_embed = EdgeEmbedding(num_currencies, num_payment_methods, currencies_embed_dim, payment_embed_dim, num_numericals)\n",
    "\n",
    "        # Combiner to do edge classification: append 2 node embeddings with edge embedding\n",
    "        edge_repr_dim = 2*hidden_dim + self.edge_embed.out_dim\n",
    "\n",
    "        # MLP for edge classification\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(edge_repr_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, graph):\n",
    "        node_features = graph.ndata['node_feats']\n",
    "        payment_currency = graph.edata['payment_currency']\n",
    "        receiving_currency = graph.edata['receiving_currency']\n",
    "        payment_method = graph.edata['payment_format']\n",
    "        edge_numericals = graph.edata['numericals']\n",
    "\n",
    "        # GCN message passing to learn node embeddings\n",
    "        h = node_features\n",
    "        for i, gcn_layer in enumerate(self.gcn_layers):\n",
    "            h = gcn_layer(graph, h)\n",
    "            if self.use_batch_norm:\n",
    "                h = self.batch_norms[i](h)\n",
    "            if i < self.num_gcn_layers - 1: # dropout between each layer except after last\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # get node embedding for each edge\n",
    "        src_nodes, dst_nodes = graph.edges()\n",
    "        src_embed = h[src_nodes]\n",
    "        dst_embed = h[dst_nodes]\n",
    "\n",
    "        # process edge features\n",
    "        edge_features = self.edge_embed(payment_currency, receiving_currency, payment_method, edge_numericals)\n",
    "\n",
    "        # concat 2 nodes embeddings with edge features to get complete edge embedding\n",
    "        edge_repr = torch.cat([src_embed, dst_embed, edge_features], dim=1)\n",
    "\n",
    "        # classify edges\n",
    "        logits = self.edge_classifier(edge_repr)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94602494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define focal CE loss since we have extreme class imbalance\n",
    "# Didn't work very well. Current implementation uses weighted normal CE\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.shape[1])\n",
    "        p_t = (probs * targets_one_hot).sum(dim=1) # probability of true class in dataset\n",
    "        focal_weight = (1 - p_t) ** self.gamma # easy examples p_t near 1, focal weight near 0\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]  # [N]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean() #default\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf43776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pass edge features as separate arguments from graph.edata\n",
    "        logits = model(graph)\n",
    "        \n",
    "        labels = graph.edata['is_laundering']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        \n",
    "        accuracy = (preds == labels).float().mean().item()\n",
    "        \n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            labels.cpu().numpy(), \n",
    "            preds.cpu().numpy(),\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), probs.cpu().numpy())\n",
    "        except:\n",
    "            auc = 0.0\n",
    "        \n",
    "        cm = confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': support,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        return metrics, preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42163b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Class weights: tensor([  1., 900.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Setup model and loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "training_graph = training_graph.to(device)\n",
    "model = GCNEdgeClassifier().to(device)\n",
    "\n",
    "train_labels = training_graph.edata['is_laundering'].long()\n",
    "############ MINORITY CLASS SCALE WEIGHT ############\n",
    "class_weights = torch.tensor([1.0, 900.0]).to(device) #place 900x weight on positive examples\n",
    "######################################################\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "# criterion = FocalLoss(alpha=class_weights, gamma=1.5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bd532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch   0 | Loss: 0.6918 | Acc: 0.9409\n",
      "  Class 1 - P: 0.0014, R: 0.1009, F1: 0.0028\n",
      "  Predicting class 1: 5.84580%\n",
      "  ✓ Saved best model (F1: 0.0028)\n",
      "Epoch   1 | Loss: 0.6726\n",
      "\n",
      "Epoch   2 | Loss: 0.6691 | Acc: 0.9416\n",
      "  Class 1 - P: 0.0014, R: 0.1000, F1: 0.0028\n",
      "  Predicting class 1: 5.77477%\n",
      "  ✓ Saved best model (F1: 0.0028)\n",
      "Epoch   3 | Loss: 0.6667\n",
      "\n",
      "Epoch   4 | Loss: 0.6622 | Acc: 0.9258\n",
      "  Class 1 - P: 0.0014, R: 0.1249, F1: 0.0028\n",
      "  Predicting class 1: 7.35313%\n",
      "Epoch   5 | Loss: 0.6506\n",
      "\n",
      "Epoch   6 | Loss: 0.6508 | Acc: 0.9097\n",
      "  Class 1 - P: 0.0018, R: 0.1899, F1: 0.0035\n",
      "  Predicting class 1: 8.97401%\n",
      "  ✓ Saved best model (F1: 0.0035)\n",
      "Epoch   7 | Loss: 0.6374\n",
      "\n",
      "Epoch   8 | Loss: 0.6366 | Acc: 0.8702\n",
      "  Class 1 - P: 0.0025, R: 0.3938, F1: 0.0050\n",
      "  Predicting class 1: 12.96579%\n",
      "  ✓ Saved best model (F1: 0.0050)\n",
      "Epoch   9 | Loss: 0.6349\n",
      "\n",
      "Epoch  10 | Loss: 0.6239 | Acc: 0.8435\n",
      "  Class 1 - P: 0.0027, R: 0.5092, F1: 0.0054\n",
      "  Predicting class 1: 15.64995%\n",
      "  ✓ Saved best model (F1: 0.0054)\n",
      "Epoch  11 | Loss: 0.6186\n",
      "\n",
      "Epoch  12 | Loss: 0.6132 | Acc: 0.8346\n",
      "  Class 1 - P: 0.0027, R: 0.5456, F1: 0.0055\n",
      "  Predicting class 1: 16.54705%\n",
      "  ✓ Saved best model (F1: 0.0055)\n",
      "Epoch  13 | Loss: 0.6044\n",
      "\n",
      "Epoch  14 | Loss: 0.5973 | Acc: 0.8268\n",
      "  Class 1 - P: 0.0028, R: 0.5772, F1: 0.0055\n",
      "  Predicting class 1: 17.32924%\n",
      "  ✓ Saved best model (F1: 0.0055)\n",
      "Epoch  15 | Loss: 0.5983\n",
      "\n",
      "Epoch  16 | Loss: 0.5851 | Acc: 0.8083\n",
      "  Class 1 - P: 0.0027, R: 0.6293, F1: 0.0054\n",
      "  Predicting class 1: 19.19021%\n",
      "Epoch  17 | Loss: 0.5797\n",
      "\n",
      "Epoch  18 | Loss: 0.5697 | Acc: 0.7764\n",
      "  Class 1 - P: 0.0026, R: 0.7021, F1: 0.0052\n",
      "  Predicting class 1: 22.38921%\n",
      "Epoch  19 | Loss: 0.5612\n",
      "\n",
      "Epoch  20 | Loss: 0.5589 | Acc: 0.7880\n",
      "  Class 1 - P: 0.0027, R: 0.6970, F1: 0.0054\n",
      "  Predicting class 1: 21.23566%\n",
      "Epoch  21 | Loss: 0.5593\n",
      "\n",
      "Epoch  22 | Loss: 0.5468 | Acc: 0.7713\n",
      "  Class 1 - P: 0.0028, R: 0.7589, F1: 0.0055\n",
      "  Predicting class 1: 22.91261%\n",
      "Epoch  23 | Loss: 0.5396\n",
      "\n",
      "Epoch  24 | Loss: 0.5349 | Acc: 0.7567\n",
      "  Class 1 - P: 0.0027, R: 0.8050, F1: 0.0055\n",
      "  Predicting class 1: 24.38075%\n",
      "Epoch  25 | Loss: 0.5197\n",
      "\n",
      "Epoch  26 | Loss: 0.5148 | Acc: 0.7958\n",
      "  Class 1 - P: 0.0032, R: 0.7831, F1: 0.0063\n",
      "  Predicting class 1: 20.46966%\n",
      "  ✓ Saved best model (F1: 0.0063)\n",
      "Epoch  27 | Loss: 0.5136\n",
      "\n",
      "Epoch  28 | Loss: 0.4986 | Acc: 0.7841\n",
      "  Class 1 - P: 0.0032, R: 0.8317, F1: 0.0064\n",
      "  Predicting class 1: 21.64750%\n",
      "  ✓ Saved best model (F1: 0.0064)\n",
      "Epoch  29 | Loss: 0.4844\n",
      "\n",
      "Epoch  30 | Loss: 0.4779 | Acc: 0.7747\n",
      "  Class 1 - P: 0.0032, R: 0.8728, F1: 0.0064\n",
      "  Predicting class 1: 22.59090%\n",
      "  ✓ Saved best model (F1: 0.0064)\n",
      "Epoch  31 | Loss: 0.4681\n",
      "\n",
      "Epoch  32 | Loss: 0.4599 | Acc: 0.7881\n",
      "  Class 1 - P: 0.0034, R: 0.8814, F1: 0.0069\n",
      "  Predicting class 1: 21.25417%\n",
      "  ✓ Saved best model (F1: 0.0069)\n",
      "Epoch  33 | Loss: 0.4592\n",
      "\n",
      "Epoch  34 | Loss: 0.4498 | Acc: 0.7749\n",
      "  Class 1 - P: 0.0034, R: 0.9130, F1: 0.0067\n",
      "  Predicting class 1: 22.57770%\n",
      "Epoch  35 | Loss: 0.4339\n",
      "\n",
      "Epoch  36 | Loss: 0.4330 | Acc: 0.7801\n",
      "  Class 1 - P: 0.0035, R: 0.9234, F1: 0.0069\n",
      "  Predicting class 1: 22.06272%\n",
      "  ✓ Saved best model (F1: 0.0069)\n",
      "Epoch  37 | Loss: 0.4356\n",
      "\n",
      "Epoch  38 | Loss: 0.4189 | Acc: 0.7768\n",
      "  Class 1 - P: 0.0035, R: 0.9373, F1: 0.0069\n",
      "  Predicting class 1: 22.39327%\n",
      "  ✓ Saved best model (F1: 0.0069)\n",
      "Epoch  39 | Loss: 0.4098\n",
      "\n",
      "Epoch  40 | Loss: 0.4044 | Acc: 0.7963\n",
      "  Class 1 - P: 0.0038, R: 0.9328, F1: 0.0076\n",
      "  Predicting class 1: 20.44682%\n",
      "  ✓ Saved best model (F1: 0.0076)\n",
      "Epoch  41 | Loss: 0.3981\n",
      "\n",
      "Epoch  42 | Loss: 0.3884 | Acc: 0.7725\n",
      "  Class 1 - P: 0.0035, R: 0.9571, F1: 0.0070\n",
      "  Predicting class 1: 22.82847%\n",
      "Epoch  43 | Loss: 0.3792\n",
      "\n",
      "Epoch  44 | Loss: 0.3808 | Acc: 0.7872\n",
      "  Class 1 - P: 0.0037, R: 0.9521, F1: 0.0074\n",
      "  Predicting class 1: 21.35962%\n",
      "Epoch  45 | Loss: 0.3707\n",
      "\n",
      "Epoch  46 | Loss: 0.3621 | Acc: 0.7818\n",
      "  Class 1 - P: 0.0037, R: 0.9609, F1: 0.0073\n",
      "  Predicting class 1: 21.89572%\n",
      "Epoch  47 | Loss: 0.3564\n",
      "\n",
      "Epoch  48 | Loss: 0.3482 | Acc: 0.7889\n",
      "  Class 1 - P: 0.0038, R: 0.9601, F1: 0.0075\n",
      "  Predicting class 1: 21.18618%\n",
      "Epoch  49 | Loss: 0.3422\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_f1 = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(training_graph)\n",
    "    loss = criterion(logits, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate every 5 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eval_logits = model(training_graph)\n",
    "            preds = eval_logits.argmax(dim=1)\n",
    "            \n",
    "            # Metrics\n",
    "            acc = (preds == train_labels).float().mean().item()\n",
    "            \n",
    "            # Class 1 metrics\n",
    "            tp = ((preds == 1) & (train_labels == 1)).sum().item()\n",
    "            fp = ((preds == 1) & (train_labels == 0)).sum().item()\n",
    "            fn = ((preds == 0) & (train_labels == 1)).sum().item()\n",
    "            \n",
    "            precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "            \n",
    "            pred_1_pct = (preds == 1).float().mean().item() * 100\n",
    "            \n",
    "        print(f\"\\nEpoch {epoch:3d} | Loss: {loss.item():.4f} | Acc: {acc:.4f}\")\n",
    "        print(f\"  Class 1 - P: {precision_1:.4f}, R: {recall_1:.4f}, F1: {f1_1:.4f}\")\n",
    "        print(f\"  Predicting class 1: {pred_1_pct:.5f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if f1_1 > best_f1:\n",
    "            best_f1 = f1_1\n",
    "            torch.save(model.state_dict(), 'best_gcn_model.pt')\n",
    "            print(f\"  Saved best model (F1: {best_f1:.4f})\")\n",
    "        \n",
    "        del eval_logits, preds\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19ea71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up training graph...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning up training graph...\")\n",
    "training_graph = training_graph.to('cpu')\n",
    "del training_graph, train_labels\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51dcd6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "Class 0: 1013872 (99.82%)\n",
      "Class 1: 1797 (0.18%)\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "Overall Accuracy: 0.8150\n",
      "AUC-ROC: 0.8297\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN:  826636  FP:  187236\n",
      "  FN:     628  TP:    1169\n",
      "\n",
      "Class 0 (Legitimate):\n",
      "  Precision: 0.9992\n",
      "  Recall: 0.8153\n",
      "\n",
      "Class 1 (Laundering):\n",
      "  Precision: 0.0062\n",
      "  Recall: 0.6505\n",
      "  F1 Score: 0.0123\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load('best_gcn_model.pt'))\n",
    "test_graph = test_graph.to(device)\n",
    "\n",
    "test_labels = test_graph.edata['is_laundering'].long()\n",
    "\n",
    "print(f\"Class 0: {(test_labels==0).sum()} ({100*(test_labels==0).float().mean():.2f}%)\")\n",
    "print(f\"Class 1: {(test_labels==1).sum()} ({100*(test_labels==1).float().mean():.2f}%)\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits = model(test_graph)\n",
    "    test_preds = test_logits.argmax(dim=1)\n",
    "    test_probs = torch.softmax(test_logits, dim=1)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    acc = (test_preds == test_labels).float().mean().item()\n",
    "    \n",
    "    tp = ((test_preds == 1) & (test_labels == 1)).sum().item()\n",
    "    fp = ((test_preds == 1) & (test_labels == 0)).sum().item()\n",
    "    fn = ((test_preds == 0) & (test_labels == 1)).sum().item()\n",
    "    tn = ((test_preds == 0) & (test_labels == 0)).sum().item()\n",
    "    \n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(test_labels.cpu().numpy(), test_probs.cpu().numpy())\n",
    "    except:\n",
    "        auc = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {tn:7d}  FP: {fp:7d}\")\n",
    "print(f\"  FN: {fn:7d}  TP: {tp:7d}\")\n",
    "print(f\"\\nClass 0 (Legitimate):\")\n",
    "print(f\"  Precision: {precision_0:.4f}\")\n",
    "print(f\"  Recall: {recall_0:.4f}\")\n",
    "print(f\"\\nClass 1 (Laundering):\")\n",
    "print(f\"  Precision: {precision_1:.4f}\")\n",
    "print(f\"  Recall: {recall_1:.4f}\")\n",
    "print(f\"  F1 Score: {f1_1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
