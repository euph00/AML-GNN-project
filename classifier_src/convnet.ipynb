{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b190d5",
   "metadata": {},
   "source": [
    "# Simple Graph Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b260530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from modules.data_loader import *\n",
    "from modules.feature_engineering import *\n",
    "from modules.visualizer import *\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2913730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run if you dont have the data downloaded this is a few gb of data\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3353db",
   "metadata": {},
   "source": [
    "## 1. Create our dataframes from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4923a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HI-Small...\n",
      "\n",
      "\n",
      "Loading transactions from: /home/linch/.cache/kagglehub/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/versions/8/HI-Small_Trans.csv\n",
      "File size: 453.6 MB\n",
      "\n",
      "Loaded 5,078,345 transactions\n",
      "Date range: 2022-09-01 00:00:00 to 2022-09-18 16:18:00\n",
      "Laundering transactions: 5,177 (0.102%)\n",
      "   transaction_id           timestamp from_bank from_account to_bank  \\\n",
      "0               0 2022-09-01 00:20:00       010    8000EBD30     010   \n",
      "1               1 2022-09-01 00:20:00     03208    8000F4580     001   \n",
      "2               2 2022-09-01 00:00:00     03209    8000F4670   03209   \n",
      "3               3 2022-09-01 00:02:00       012    8000F5030     012   \n",
      "4               4 2022-09-01 00:06:00       010    8000F5200     010   \n",
      "\n",
      "  to_account  amount_received receiving_currency  amount_paid  \\\n",
      "0  8000EBD30          3697.34          US Dollar      3697.34   \n",
      "1  8000F5340             0.01          US Dollar         0.01   \n",
      "2  8000F4670         14675.57          US Dollar     14675.57   \n",
      "3  8000F5030          2806.97          US Dollar      2806.97   \n",
      "4  8000F5200         36682.97          US Dollar     36682.97   \n",
      "\n",
      "  payment_currency payment_format  is_laundering  \n",
      "0        US Dollar   Reinvestment              0  \n",
      "1        US Dollar         Cheque              0  \n",
      "2        US Dollar   Reinvestment              0  \n",
      "3        US Dollar   Reinvestment              0  \n",
      "4        US Dollar   Reinvestment              0  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"HI-Small\"\n",
    "\n",
    "print(f\"Loading {dataset_name}...\\n\")\n",
    "trans_df = load_transactions(dataset_size=dataset_name)\n",
    "# accounts_df = load_accounts(dataset_size=dataset_name)\n",
    "# patterns_df = load_patterns(dataset_size=dataset_name)\n",
    "print(trans_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428fcaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert all currencies to USD for normaliztion\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trans_df \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_currency_to_USD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(trans_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/AML-GNN-project/classifier_src/modules/feature_engineering.py:46\u001b[0m, in \u001b[0;36mconvert_currency_to_USD\u001b[0;34m(trans_df)\u001b[0m\n\u001b[1;32m     40\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_received\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_received\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m exchange_rates_to_usd\u001b[38;5;241m.\u001b[39mget(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreceiving_currency\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     42\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert amount_paid to USD\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_paid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamount_paid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexchange_rates_to_usd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_currency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/AML-GNN-project/classifier_src/modules/feature_engineering.py:47\u001b[0m, in \u001b[0;36mconvert_currency_to_USD.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     40\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_received\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_received\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m exchange_rates_to_usd\u001b[38;5;241m.\u001b[39mget(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreceiving_currency\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     42\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert amount_paid to USD\u001b[39;00m\n\u001b[1;32m     46\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_paid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_paid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m exchange_rates_to_usd\u001b[38;5;241m.\u001b[39mget(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_currency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     48\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/series.py:993\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 993\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aml_project_new/lib/python3.8/site-packages/pandas/core/indexing.py:2623\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2614\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   2617\u001b[0m ):\n\u001b[1;32m   2618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a set as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m-> 2623\u001b[0m     \u001b[38;5;28;43misinstance\u001b[39;49m(key, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   2625\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   2626\u001b[0m ):\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a dict as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2629\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert all currencies to USD for normaliztion\n",
    "\n",
    "trans_df = convert_currency_to_USD(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcfde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized']\n"
     ]
    }
   ],
   "source": [
    "# compute sinusoidal temporal encodings and normalized unix timestamp\n",
    "\n",
    "trans_df = temporal_encoding(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id']\n"
     ]
    }
   ],
   "source": [
    "# give each currency and payment method a unique integer ID\n",
    "\n",
    "trans_df = encode_currency_ids(trans_df)\n",
    "trans_df = encode_payment_format_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n"
     ]
    }
   ],
   "source": [
    "# give each account a unique integer ID\n",
    "\n",
    "trans_df, account_to_id, id_to_account = encode_account_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = normalize_amounts(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6kt67xtz4c9",
   "metadata": {},
   "source": [
    "## 2. Temporal train/test split and account statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0nzrvv4m8bqr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL TRAIN/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "Train Set:\n",
      "  Date range: 2022-09-01 00:00:00 to 2022-09-08 16:12:00\n",
      "  Transactions: 4,062,676\n",
      "  Laundering: 3,380 (0.083%)\n",
      "\n",
      "Test Set:\n",
      "  Date range: 2022-09-08 16:12:00 to 2022-09-18 16:18:00\n",
      "  Transactions: 1,015,669\n",
      "  Laundering: 1,797 (0.177%)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data temporally: first 80% for training, last 20% for testing\n",
    "\n",
    "train_df, test_df = temporal_train_test_split(trans_df, train_ratio=0.8)\n",
    "\n",
    "import gc\n",
    "del trans_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n",
      "   transaction_id  timestamp from_bank from_account to_bank to_account  \\\n",
      "0          316720 2022-09-01      0121    8123FB9B0    0121  8123FB9B0   \n",
      "\n",
      "   amount_received receiving_currency  amount_paid payment_currency  \\\n",
      "0        -1.529313        Saudi Riyal    -1.529313      Saudi Riyal   \n",
      "\n",
      "  payment_format  is_laundering  hour_sin  hour_cos  time_normalized  \\\n",
      "0   Reinvestment              0       0.0       1.0         -1.37763   \n",
      "\n",
      "  payment_currency_id receiving_currency_id payment_format_id  \\\n",
      "0                   8                     8                 5   \n",
      "\n",
      "   from_account_id  to_account_id  \n",
      "0           458857         458857  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.tolist())\n",
    "print(train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb74eed",
   "metadata": {},
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_df(df):\n",
    "    source_accounts = df['from_account_id'].values\n",
    "    dest_accounts = df['to_account_id'].values\n",
    "\n",
    "    graph = dgl.graph((source_accounts, dest_accounts))\n",
    "    print(f\"Number of nodes: {graph.num_nodes()}\")\n",
    "    print(f\"Number of edges: {graph.num_edges()}\")\n",
    "    # EDGE FEATS\n",
    "\n",
    "    # bookkeeping\n",
    "    graph.edata['transaction_id'] = torch.tensor(df['transaction_id'].values, dtype=torch.long)\n",
    "    graph.edata['is_laundering'] = torch.tensor(df['is_laundering'].values, dtype=torch.long) #not used in training\n",
    "\n",
    "    # numericals: we can include them directly\n",
    "    numericals = ['amount_received', 'amount_paid', 'hour_sin', 'hour_cos', 'time_normalized']\n",
    "    edge_numerical = torch.tensor(df[numericals].values, dtype=torch.float32)\n",
    "    graph.edata['numericals'] = edge_numerical\n",
    "\n",
    "    # categoricals: we use learned embeddings. This makes them transductive, but that is ok because currencies and payment types don't change often\n",
    "\n",
    "    payment_currency = torch.tensor(df['payment_currency_id'].values, dtype=torch.long)\n",
    "    receiving_currency = torch.tensor(df['receiving_currency_id'].values, dtype=torch.long)\n",
    "    payment_format = torch.tensor(df['payment_format_id'].values, dtype=torch.long)\n",
    "    graph.edata['payment_currency'] = payment_currency\n",
    "    graph.edata['receiving_currency'] = receiving_currency\n",
    "    graph.edata['payment_format'] = payment_format\n",
    "\n",
    "    # NODE FEATS\n",
    "\n",
    "    # For each account, we compute their in and outdegrees\n",
    "    # We zero-center and log transform this value and normalize by the graph's average transformed in and outdegrees so that our model is not sensitive to the graph size. Our training graph has 80% of the edges, so naturally the raw degree counts will be higher than the testing graph.\n",
    "    indegrees = graph.in_degrees().float()\n",
    "    outdegrees = graph.out_degrees().float()\n",
    "\n",
    "    log_indegrees = torch.log(indegrees + 1)\n",
    "    log_outdegrees = torch.log(outdegrees + 1)\n",
    "\n",
    "    avg_log_indegree = log_indegrees.mean()\n",
    "    avg_log_outdegree = log_outdegrees.mean()\n",
    "\n",
    "    # zc and normalize\n",
    "    normalized_indegree = (log_indegrees - avg_log_indegree) / avg_log_indegree\n",
    "    normalized_outdegree = (log_outdegrees - avg_log_outdegree) / avg_log_outdegree\n",
    "\n",
    "    node_features = torch.stack([normalized_indegree, normalized_outdegree], dim=1)\n",
    "    graph.ndata['node_feats'] = node_features\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e2fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 515080\n",
      "Number of edges: 4062676\n",
      "Number of nodes: 515080\n",
      "Number of edges: 1015669\n"
     ]
    }
   ],
   "source": [
    "training_graph = build_graph_from_df(train_df)\n",
    "test_graph = build_graph_from_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Node Features (normalized degrees) =====\n",
      "\n",
      "----- Node Degrees (first 10 nodes) -----\n",
      "Node 0:\n",
      "  In-degree:  545  → normalized: +3.1889\n",
      "  Out-degree: 132597 → normalized: +7.6085\n",
      "Node 1:\n",
      "  In-degree:  328  → normalized: +2.8523\n",
      "  Out-degree: 80883 → normalized: +7.2477\n",
      "Node 2:\n",
      "  In-degree:  55  → normalized: +1.6754\n",
      "  Out-degree: 14711 → normalized: +6.0038\n",
      "Node 3:\n",
      "  In-degree:  53  → normalized: +1.6512\n",
      "  Out-degree: 10810 → normalized: +5.7789\n",
      "Node 4:\n",
      "  In-degree:  61  → normalized: +1.7430\n",
      "  Out-degree: 13641 → normalized: +5.9487\n",
      "\n",
      "===== Edge Features =====\n",
      "Numerical features shape: torch.Size([4062676, 5])\n",
      "Categorical feature 1 shape: torch.Size([4062676])\n",
      "Categorical feature 2 shape: torch.Size([4062676])\n",
      "Categorical feature 3 shape: torch.Size([4062676])\n",
      "\n",
      "===== Sample Edge Features =====\n",
      "Edge 0: 458857 -> 458857\n",
      "  Numerical: [-1.5293132066726685, -1.529313087463379, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 8, receiving currency: 8 , payment format: 5\n",
      "Edge 1: 236631 -> 236631\n",
      "  Numerical: [0.3639656603336334, 0.3639656603336334, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 5\n",
      "Edge 2: 244237 -> 229946\n",
      "  Numerical: [-0.9314213991165161, -0.9314213991165161, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 4\n",
      "Edge 3: 20992 -> 20994\n",
      "  Numerical: [0.9639478325843811, 0.9639478325843811, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 6\n",
      "Edge 4: 19729 -> 19729\n",
      "  Numerical: [-1.5437047481536865, -1.5437047481536865, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 5\n"
     ]
    }
   ],
   "source": [
    "# Check graph sanity\n",
    "\n",
    "print(\"\\n===== Node Features (normalized degrees) =====\")\n",
    "print(\"\\n----- Node Degrees (first 10 nodes) -----\")\n",
    "indegrees = training_graph.in_degrees().float()\n",
    "outdegrees = training_graph.out_degrees().float()\n",
    "\n",
    "for i in range(5):\n",
    "    norm_in = training_graph.ndata['node_feats'][i, 0].item()\n",
    "    norm_out = training_graph.ndata['node_feats'][i, 1].item()\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  In-degree:  {indegrees[i].item():.0f}  → normalized: {norm_in:+.4f}\")\n",
    "    print(f\"  Out-degree: {outdegrees[i].item():.0f} → normalized: {norm_out:+.4f}\")\n",
    "\n",
    "print(\"\\n===== Edge Features =====\")\n",
    "print(f\"Numerical features shape: {training_graph.edata['numericals'].shape}\")\n",
    "print(f\"Categorical feature 1 shape: {training_graph.edata['payment_currency'].shape}\")\n",
    "print(f\"Categorical feature 2 shape: {training_graph.edata['receiving_currency'].shape}\")\n",
    "print(f\"Categorical feature 3 shape: {training_graph.edata['payment_format'].shape}\")\n",
    "\n",
    "print(\"\\n===== Sample Edge Features =====\")\n",
    "for i in range(5):\n",
    "    src, dst = training_graph.edges()\n",
    "    print(f\"Edge {i}: {src[i].item()} -> {dst[i].item()}\")\n",
    "    print(f\"  Numerical: {training_graph.edata['numericals'][i].tolist()}\")\n",
    "    print(f\"  payment currency: {training_graph.edata['payment_currency'][i].item()}, receiving currency: {training_graph.edata['receiving_currency'][i].item()} , payment format: {training_graph.edata['payment_format'][i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3610b26",
   "metadata": {},
   "source": [
    "# Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have both categorical and numerical edge features, we need a class to process and combine them\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_currencies=15, \n",
    "            num_payment_methods=7, \n",
    "            currencies_embed_dim=8, \n",
    "            payment_embed_dim=4, \n",
    "            num_numericals=5):\n",
    "        super().__init__()\n",
    "        self.currencies_embed = nn.Embedding(num_currencies, currencies_embed_dim)\n",
    "        self.payment_embed = nn.Embedding(num_payment_methods, payment_embed_dim)\n",
    "\n",
    "        # each edge has payment currency, receiving currency, payment method and numericals\n",
    "        self.out_dim = currencies_embed_dim + currencies_embed_dim + payment_embed_dim + num_numericals\n",
    "\n",
    "    def forward(self, payment_curr, receiving_curr, payment_method, numericals):\n",
    "        payment_curr_embed = self.currencies_embed(payment_curr)\n",
    "        receiving_curr_embed = self.currencies_embed(receiving_curr)\n",
    "        payment_method_embed = self.payment_embed(payment_method)\n",
    "\n",
    "        edge_feats = torch.cat([payment_curr_embed, receiving_curr_embed, payment_method_embed, numericals], dim=1)\n",
    "        return edge_feats\n",
    "\n",
    "# this returns our GNN-ready edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message passing layers\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, residual=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.residual = residual\n",
    "\n",
    "        if residual and input_dim != output_dim:\n",
    "            self.res_linear = nn.Linear(input_dim, output_dim) # project residual if diff dims\n",
    "        elif residual:\n",
    "            self.res_linear = nn.Identity()\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m': edges.src['Wh'] * edges.src['norm']} # normalize by source degree first\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        return {'h': torch.sum(nodes.mailbox['m'], dim=1)}\n",
    "    \n",
    "    def forward(self, graph, node_feats):\n",
    "        with graph.local_scope():\n",
    "            h_in = node_feats\n",
    "\n",
    "            graph.ndata['Wh'] = self.linear(node_feats) # W is linear transform, h is node feats\n",
    "\n",
    "            degs = graph.in_degrees().float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5).unsqueeze(1)\n",
    "            graph.ndata['norm'] = norm\n",
    "\n",
    "            graph.update_all(self.message_func, self.reduce_func)\n",
    "\n",
    "            h_out = graph.ndata['h'] * norm # then normalize by destination degree\n",
    "\n",
    "            h_out = F.relu(h_out)\n",
    "            if self.residual:\n",
    "                return h_out + self.res_linear(h_in)\n",
    "            else:\n",
    "                return h_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classifier neural network\n",
    "\n",
    "class GCNEdgeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                # node params\n",
    "                node_in_feats=2, # log transformed and normalized indegree outdegree\n",
    "                hidden_dim=64,\n",
    "                num_gcn_layers=3,\n",
    "\n",
    "                # for EdgeEmbedding\n",
    "                num_currencies=15,\n",
    "                num_payment_methods=7,\n",
    "                currencies_embed_dim=8,\n",
    "                payment_embed_dim=4,\n",
    "                num_numericals=5,\n",
    "\n",
    "                # output classes\n",
    "                num_classes=2,\n",
    "\n",
    "                # regularization\n",
    "                dropout=0.2,\n",
    "                use_batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_gcn_layers = num_gcn_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(node_in_feats, hidden_dim, residual=False)) # no residual for layer 1 since in dim and out dim dont match, simpler to just skip\n",
    "        for i in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(hidden_dim, hidden_dim, residual=True))\n",
    "        if use_batch_norm: # create batch norm layers if using them\n",
    "            self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for i in range(num_gcn_layers)])\n",
    "\n",
    "        # Edge Feature processor\n",
    "        self.edge_embed = EdgeEmbedding(num_currencies, num_payment_methods, currencies_embed_dim, payment_embed_dim, num_numericals)\n",
    "\n",
    "        # Combiner to do edge classification: append 2 node embeddings with edge embedding\n",
    "        edge_repr_dim = 2*hidden_dim + self.edge_embed.out_dim\n",
    "\n",
    "        # MLP for edge classification\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(edge_repr_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, graph):\n",
    "        node_features = graph.ndata['node_feats']\n",
    "        payment_currency = graph.edata['payment_currency']\n",
    "        receiving_currency = graph.edata['receiving_currency']\n",
    "        payment_method = graph.edata['payment_format']\n",
    "        edge_numericals = graph.edata['numericals']\n",
    "\n",
    "        # GCN message passing to learn node embeddings\n",
    "        h = node_features\n",
    "        for i, gcn_layer in enumerate(self.gcn_layers):\n",
    "            h = gcn_layer(graph, h)\n",
    "            if self.use_batch_norm:\n",
    "                h = self.batch_norms[i](h)\n",
    "            if i < self.num_gcn_layers - 1: # dropout between each layer except after last\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # get node embedding for each edge\n",
    "        src_nodes, dst_nodes = graph.edges()\n",
    "        src_embed = h[src_nodes]\n",
    "        dst_embed = h[dst_nodes]\n",
    "\n",
    "        # process edge features\n",
    "        edge_features = self.edge_embed(payment_currency, receiving_currency, payment_method, edge_numericals)\n",
    "\n",
    "        # concat 2 nodes embeddings with edge features to get complete edge embedding\n",
    "        edge_repr = torch.cat([src_embed, dst_embed, edge_features], dim=1)\n",
    "\n",
    "        # classify edges\n",
    "        logits = self.edge_classifier(edge_repr)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94602494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define focal CE loss since we have extreme class imbalance\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.shape[1])\n",
    "        p_t = (probs * targets_one_hot).sum(dim=1) # probability of true class in dataset\n",
    "        focal_weight = (1 - p_t) ** self.gamma # easy examples p_t near 1, focal weight near 0\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]  # [N]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean() #default\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf43776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pass edge features as separate arguments from graph.edata\n",
    "        logits = model(graph)\n",
    "        \n",
    "        labels = graph.edata['is_laundering']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        \n",
    "        accuracy = (preds == labels).float().mean().item()\n",
    "        \n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            labels.cpu().numpy(), \n",
    "            preds.cpu().numpy(),\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), probs.cpu().numpy())\n",
    "        except:\n",
    "            auc = 0.0\n",
    "        \n",
    "        cm = confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': support,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        return metrics, preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42163b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Class weights: tensor([  1., 600.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Setup model and loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "training_graph = training_graph.to(device)\n",
    "model = GCNEdgeClassifier().to(device)\n",
    "\n",
    "train_labels = training_graph.edata['is_laundering'].long()\n",
    "############ MINORITY CLASS SCALE WEIGHT ############\n",
    "class_weights = torch.tensor([1.0, 900.0]).to(device) #place 600x weight on positive examples\n",
    "######################################################\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "# criterion = FocalLoss(alpha=class_weights, gamma=1.5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bd532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch   0 | Loss: 0.6943 | Acc: 0.9925\n",
      "  Class 1 - P: 0.0018, R: 0.0148, F1: 0.0033\n",
      "  Predicting class 1: 0.66700%\n",
      "  ✓ Saved best model (F1: 0.0033)\n",
      "Epoch   1 | Loss: 0.6632\n",
      "\n",
      "Epoch   2 | Loss: 0.6419 | Acc: 0.9992\n",
      "  Class 1 - P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "  Predicting class 1: 0.00000%\n",
      "Epoch   3 | Loss: 0.6315\n",
      "\n",
      "Epoch   4 | Loss: 0.6163 | Acc: 0.9991\n",
      "  Class 1 - P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "  Predicting class 1: 0.00357%\n",
      "Epoch   5 | Loss: 0.6121\n",
      "\n",
      "Epoch   6 | Loss: 0.6086 | Acc: 0.9643\n",
      "  Class 1 - P: 0.0014, R: 0.0598, F1: 0.0028\n",
      "  Predicting class 1: 3.50057%\n",
      "Epoch   7 | Loss: 0.6089\n",
      "\n",
      "Epoch   8 | Loss: 0.6031 | Acc: 0.9466\n",
      "  Class 1 - P: 0.0015, R: 0.0935, F1: 0.0029\n",
      "  Predicting class 1: 5.27404%\n",
      "Epoch   9 | Loss: 0.5949\n",
      "\n",
      "Epoch  10 | Loss: 0.5950 | Acc: 0.9408\n",
      "  Class 1 - P: 0.0015, R: 0.1024, F1: 0.0029\n",
      "  Predicting class 1: 5.84952%\n",
      "Epoch  11 | Loss: 0.5883\n",
      "\n",
      "Epoch  12 | Loss: 0.5842 | Acc: 0.9348\n",
      "  Class 1 - P: 0.0017, R: 0.1320, F1: 0.0034\n",
      "  Predicting class 1: 6.45663%\n",
      "  ✓ Saved best model (F1: 0.0034)\n",
      "Epoch  13 | Loss: 0.5780\n",
      "\n",
      "Epoch  14 | Loss: 0.5748 | Acc: 0.9320\n",
      "  Class 1 - P: 0.0022, R: 0.1757, F1: 0.0043\n",
      "  Predicting class 1: 6.75011%\n",
      "  ✓ Saved best model (F1: 0.0043)\n",
      "Epoch  15 | Loss: 0.5747\n",
      "\n",
      "Epoch  16 | Loss: 0.5671 | Acc: 0.9364\n",
      "  Class 1 - P: 0.0026, R: 0.2003, F1: 0.0052\n",
      "  Predicting class 1: 6.31168%\n",
      "  ✓ Saved best model (F1: 0.0052)\n",
      "Epoch  17 | Loss: 0.5607\n",
      "\n",
      "Epoch  18 | Loss: 0.5595 | Acc: 0.9370\n",
      "  Class 1 - P: 0.0033, R: 0.2453, F1: 0.0064\n",
      "  Predicting class 1: 6.26004%\n",
      "  ✓ Saved best model (F1: 0.0064)\n",
      "Epoch  19 | Loss: 0.5529\n",
      "\n",
      "Epoch  20 | Loss: 0.5486 | Acc: 0.9236\n",
      "  Class 1 - P: 0.0039, R: 0.3615, F1: 0.0078\n",
      "  Predicting class 1: 7.61978%\n",
      "  ✓ Saved best model (F1: 0.0078)\n",
      "Epoch  21 | Loss: 0.5452\n",
      "\n",
      "Epoch  22 | Loss: 0.5399 | Acc: 0.9033\n",
      "  Class 1 - P: 0.0041, R: 0.4707, F1: 0.0080\n",
      "  Predicting class 1: 9.66053%\n",
      "  ✓ Saved best model (F1: 0.0080)\n",
      "Epoch  23 | Loss: 0.5337\n",
      "\n",
      "Epoch  24 | Loss: 0.5361 | Acc: 0.8979\n",
      "  Class 1 - P: 0.0041, R: 0.5062, F1: 0.0082\n",
      "  Predicting class 1: 10.20812%\n",
      "  ✓ Saved best model (F1: 0.0082)\n",
      "Epoch  25 | Loss: 0.5260\n",
      "\n",
      "Epoch  26 | Loss: 0.5219 | Acc: 0.9058\n",
      "  Class 1 - P: 0.0043, R: 0.4891, F1: 0.0086\n",
      "  Predicting class 1: 9.41606%\n",
      "  ✓ Saved best model (F1: 0.0086)\n",
      "Epoch  27 | Loss: 0.5125\n",
      "\n",
      "Epoch  28 | Loss: 0.5111 | Acc: 0.9102\n",
      "  Class 1 - P: 0.0049, R: 0.5243, F1: 0.0096\n",
      "  Predicting class 1: 8.98573%\n",
      "  ✓ Saved best model (F1: 0.0096)\n",
      "Epoch  29 | Loss: 0.5050\n",
      "\n",
      "Epoch  30 | Loss: 0.5009 | Acc: 0.9102\n",
      "  Class 1 - P: 0.0054, R: 0.5817, F1: 0.0107\n",
      "  Predicting class 1: 8.99149%\n",
      "  ✓ Saved best model (F1: 0.0107)\n",
      "Epoch  31 | Loss: 0.4989\n",
      "\n",
      "Epoch  32 | Loss: 0.4916 | Acc: 0.9231\n",
      "  Class 1 - P: 0.0064, R: 0.5893, F1: 0.0126\n",
      "  Predicting class 1: 7.70438%\n",
      "  ✓ Saved best model (F1: 0.0126)\n",
      "Epoch  33 | Loss: 0.4828\n",
      "\n",
      "Epoch  34 | Loss: 0.4747 | Acc: 0.9445\n",
      "  Class 1 - P: 0.0083, R: 0.5536, F1: 0.0163\n",
      "  Predicting class 1: 5.56193%\n",
      "  ✓ Saved best model (F1: 0.0163)\n",
      "Epoch  35 | Loss: 0.4687\n",
      "\n",
      "Epoch  36 | Loss: 0.4675 | Acc: 0.9348\n",
      "  Class 1 - P: 0.0077, R: 0.6053, F1: 0.0152\n",
      "  Predicting class 1: 6.54000%\n",
      "Epoch  37 | Loss: 0.4600\n",
      "\n",
      "Epoch  38 | Loss: 0.4569 | Acc: 0.9113\n",
      "  Class 1 - P: 0.0064, R: 0.6817, F1: 0.0126\n",
      "  Predicting class 1: 8.89911%\n",
      "Epoch  39 | Loss: 0.4469\n",
      "\n",
      "Epoch  40 | Loss: 0.4377 | Acc: 0.9182\n",
      "  Class 1 - P: 0.0067, R: 0.6636, F1: 0.0133\n",
      "  Predicting class 1: 8.21006%\n",
      "Epoch  41 | Loss: 0.4315\n",
      "\n",
      "Epoch  42 | Loss: 0.4310 | Acc: 0.9101\n",
      "  Class 1 - P: 0.0064, R: 0.6911, F1: 0.0126\n",
      "  Predicting class 1: 9.02462%\n",
      "Epoch  43 | Loss: 0.4207\n",
      "\n",
      "Epoch  44 | Loss: 0.4137 | Acc: 0.8776\n",
      "  Class 1 - P: 0.0053, R: 0.7867, F1: 0.0106\n",
      "  Predicting class 1: 12.28407%\n",
      "Epoch  45 | Loss: 0.4089\n",
      "\n",
      "Epoch  46 | Loss: 0.3977 | Acc: 0.9110\n",
      "  Class 1 - P: 0.0068, R: 0.7337, F1: 0.0135\n",
      "  Predicting class 1: 8.93578%\n",
      "Epoch  47 | Loss: 0.3918\n",
      "\n",
      "Epoch  48 | Loss: 0.3910 | Acc: 0.9239\n",
      "  Class 1 - P: 0.0078, R: 0.7186, F1: 0.0155\n",
      "  Predicting class 1: 7.64142%\n",
      "Epoch  49 | Loss: 0.3827\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_f1 = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(training_graph)\n",
    "    loss = criterion(logits, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate every 5 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eval_logits = model(training_graph)\n",
    "            preds = eval_logits.argmax(dim=1)\n",
    "            \n",
    "            # Metrics\n",
    "            acc = (preds == train_labels).float().mean().item()\n",
    "            \n",
    "            # Class 1 metrics\n",
    "            tp = ((preds == 1) & (train_labels == 1)).sum().item()\n",
    "            fp = ((preds == 1) & (train_labels == 0)).sum().item()\n",
    "            fn = ((preds == 0) & (train_labels == 1)).sum().item()\n",
    "            \n",
    "            precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "            \n",
    "            pred_1_pct = (preds == 1).float().mean().item() * 100\n",
    "            \n",
    "        print(f\"\\nEpoch {epoch:3d} | Loss: {loss.item():.4f} | Acc: {acc:.4f}\")\n",
    "        print(f\"  Class 1 - P: {precision_1:.4f}, R: {recall_1:.4f}, F1: {f1_1:.4f}\")\n",
    "        print(f\"  Predicting class 1: {pred_1_pct:.5f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if f1_1 > best_f1:\n",
    "            best_f1 = f1_1\n",
    "            torch.save(model.state_dict(), 'best_gcn_model.pt')\n",
    "            print(f\"  ✓ Saved best model (F1: {best_f1:.4f})\")\n",
    "        \n",
    "        del eval_logits, preds\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ea71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up training graph...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning up training graph...\")\n",
    "training_graph = training_graph.to('cpu')\n",
    "del training_graph, train_labels\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcd6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "Class 0: 1013872 (99.82%)\n",
      "Class 1: 1797 (0.18%)\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "Overall Accuracy: 0.9509\n",
      "AUC-ROC: 0.7298\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN:  965401  FP:   48471\n",
      "  FN:    1389  TP:     408\n",
      "\n",
      "Class 0 (Legitimate):\n",
      "  Precision: 0.9986\n",
      "  Recall: 0.9522\n",
      "\n",
      "Class 1 (Laundering):\n",
      "  Precision: 0.0083\n",
      "  Recall: 0.2270\n",
      "  F1 Score: 0.0161\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load('best_gcn_model.pt'))\n",
    "test_graph = test_graph.to(device)\n",
    "\n",
    "test_labels = test_graph.edata['is_laundering'].long()\n",
    "\n",
    "print(f\"Class 0: {(test_labels==0).sum()} ({100*(test_labels==0).float().mean():.2f}%)\")\n",
    "print(f\"Class 1: {(test_labels==1).sum()} ({100*(test_labels==1).float().mean():.2f}%)\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits = model(test_graph)\n",
    "    test_preds = test_logits.argmax(dim=1)\n",
    "    test_probs = torch.softmax(test_logits, dim=1)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    acc = (test_preds == test_labels).float().mean().item()\n",
    "    \n",
    "    tp = ((test_preds == 1) & (test_labels == 1)).sum().item()\n",
    "    fp = ((test_preds == 1) & (test_labels == 0)).sum().item()\n",
    "    fn = ((test_preds == 0) & (test_labels == 1)).sum().item()\n",
    "    tn = ((test_preds == 0) & (test_labels == 0)).sum().item()\n",
    "    \n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(test_labels.cpu().numpy(), test_probs.cpu().numpy())\n",
    "    except:\n",
    "        auc = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {tn:7d}  FP: {fp:7d}\")\n",
    "print(f\"  FN: {fn:7d}  TP: {tp:7d}\")\n",
    "print(f\"\\nClass 0 (Legitimate):\")\n",
    "print(f\"  Precision: {precision_0:.4f}\")\n",
    "print(f\"  Recall: {recall_0:.4f}\")\n",
    "print(f\"\\nClass 1 (Laundering):\")\n",
    "print(f\"  Precision: {precision_1:.4f}\")\n",
    "print(f\"  Recall: {recall_1:.4f}\")\n",
    "print(f\"  F1 Score: {f1_1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
