{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b190d5",
   "metadata": {},
   "source": [
    "# ANALYSIS OF GRAPH CLUSTERING WITH METIS TO ISOLATE LAUNDERING SUBGRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b260530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from modules.data_loader import *\n",
    "from modules.feature_engineering import *\n",
    "from modules.visualizer import *\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2913730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run if you dont have the data downloaded this is a few gb of data\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3353db",
   "metadata": {},
   "source": [
    "## 1. Create our dataframes from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4923a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HI-Small...\n",
      "\n",
      "\n",
      "Loading transactions from: /home/linch/.cache/kagglehub/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/versions/8/HI-Small_Trans.csv\n",
      "File size: 453.6 MB\n",
      "\n",
      "Loaded 5,078,345 transactions\n",
      "Date range: 2022-09-01 00:00:00 to 2022-09-18 16:18:00\n",
      "Laundering transactions: 5,177 (0.102%)\n",
      "   transaction_id           timestamp from_bank from_account to_bank  \\\n",
      "0               0 2022-09-01 00:20:00       010    8000EBD30     010   \n",
      "1               1 2022-09-01 00:20:00     03208    8000F4580     001   \n",
      "2               2 2022-09-01 00:00:00     03209    8000F4670   03209   \n",
      "3               3 2022-09-01 00:02:00       012    8000F5030     012   \n",
      "4               4 2022-09-01 00:06:00       010    8000F5200     010   \n",
      "\n",
      "  to_account  amount_received receiving_currency  amount_paid  \\\n",
      "0  8000EBD30          3697.34          US Dollar      3697.34   \n",
      "1  8000F5340             0.01          US Dollar         0.01   \n",
      "2  8000F4670         14675.57          US Dollar     14675.57   \n",
      "3  8000F5030          2806.97          US Dollar      2806.97   \n",
      "4  8000F5200         36682.97          US Dollar     36682.97   \n",
      "\n",
      "  payment_currency payment_format  is_laundering  \n",
      "0        US Dollar   Reinvestment              0  \n",
      "1        US Dollar         Cheque              0  \n",
      "2        US Dollar   Reinvestment              0  \n",
      "3        US Dollar   Reinvestment              0  \n",
      "4        US Dollar   Reinvestment              0  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"HI-Small\"\n",
    "\n",
    "print(f\"Loading {dataset_name}...\\n\")\n",
    "trans_df = load_transactions(dataset_size=dataset_name)\n",
    "# accounts_df = load_accounts(dataset_size=dataset_name)\n",
    "# patterns_df = load_patterns(dataset_size=dataset_name)\n",
    "print(trans_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428fcaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering']\n"
     ]
    }
   ],
   "source": [
    "# convert all currencies to USD for normaliztion\n",
    "\n",
    "trans_df = convert_currency_to_USD(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabcfde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized']\n"
     ]
    }
   ],
   "source": [
    "# compute sinusoidal temporal encodings and normalized unix timestamp\n",
    "\n",
    "trans_df = temporal_encoding(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fcd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id']\n"
     ]
    }
   ],
   "source": [
    "# give each currency and payment method a unique integer ID\n",
    "\n",
    "trans_df = encode_currency_ids(trans_df)\n",
    "trans_df = encode_payment_format_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ac1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n"
     ]
    }
   ],
   "source": [
    "# give each account a unique integer ID\n",
    "\n",
    "trans_df, account_to_id, id_to_account = encode_account_ids(trans_df)\n",
    "print(trans_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd99d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = normalize_amounts(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6kt67xtz4c9",
   "metadata": {},
   "source": [
    "## 2. Temporal train/test split and account statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0nzrvv4m8bqr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL TRAIN/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "Train Set:\n",
      "  Date range: 2022-09-01 00:00:00 to 2022-09-08 16:12:00\n",
      "  Transactions: 4,062,676\n",
      "  Laundering: 3,380 (0.083%)\n",
      "\n",
      "Test Set:\n",
      "  Date range: 2022-09-08 16:12:00 to 2022-09-18 16:18:00\n",
      "  Transactions: 1,015,669\n",
      "  Laundering: 1,797 (0.177%)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data temporally: first 80% for training, last 20% for testing\n",
    "\n",
    "train_df, test_df = temporal_train_test_split(trans_df, train_ratio=0.8)\n",
    "\n",
    "import gc\n",
    "del trans_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a427ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_id', 'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account', 'amount_received', 'receiving_currency', 'amount_paid', 'payment_currency', 'payment_format', 'is_laundering', 'hour_sin', 'hour_cos', 'time_normalized', 'payment_currency_id', 'receiving_currency_id', 'payment_format_id', 'from_account_id', 'to_account_id']\n",
      "   transaction_id  timestamp from_bank from_account to_bank to_account  \\\n",
      "0          316720 2022-09-01      0121    8123FB9B0    0121  8123FB9B0   \n",
      "\n",
      "   amount_received receiving_currency  amount_paid payment_currency  \\\n",
      "0        -1.529313        Saudi Riyal    -1.529313      Saudi Riyal   \n",
      "\n",
      "  payment_format  is_laundering  hour_sin  hour_cos  time_normalized  \\\n",
      "0   Reinvestment              0       0.0       1.0         -1.37763   \n",
      "\n",
      "  payment_currency_id receiving_currency_id payment_format_id  \\\n",
      "0                   8                     8                 5   \n",
      "\n",
      "   from_account_id  to_account_id  \n",
      "0           458857         458857  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.tolist())\n",
    "print(train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb74eed",
   "metadata": {},
   "source": [
    "# Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1bd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_df(df):\n",
    "    source_accounts = df['from_account_id'].values\n",
    "    dest_accounts = df['to_account_id'].values\n",
    "\n",
    "    graph = dgl.graph((source_accounts, dest_accounts))\n",
    "    print(f\"Number of nodes: {graph.num_nodes()}\")\n",
    "    print(f\"Number of edges: {graph.num_edges()}\")\n",
    "    # EDGE FEATS\n",
    "\n",
    "    # bookkeeping\n",
    "    graph.edata['transaction_id'] = torch.tensor(df['transaction_id'])\n",
    "\n",
    "    # numericals: we can include them directly\n",
    "    numericals = ['amount_received', 'amount_paid', 'hour_sin', 'hour_cos', 'time_normalized']\n",
    "    edge_numerical = torch.tensor(df[numericals].values, dtype=torch.float32)\n",
    "    graph.edata['numericals'] = edge_numerical\n",
    "\n",
    "    # categoricals: we use learned embeddings. This makes them transductive, but that is ok because currencies and payment types don't change often\n",
    "\n",
    "    payment_currency = torch.tensor(df['payment_currency_id'])\n",
    "    receiving_currency = torch.tensor(df['receiving_currency_id'])\n",
    "    payment_format = torch.tensor(df['payment_format_id'])\n",
    "    graph.edata['payment_currency'] = payment_currency\n",
    "    graph.edata['receiving_currency'] = receiving_currency\n",
    "    graph.edata['payment_format'] = payment_format\n",
    "\n",
    "    # NODE FEATS\n",
    "\n",
    "    # For each account, we compute their in and outdegrees\n",
    "    # We zero-center and log transform this value and normalize by the graph's average transformed in and outdegrees so that our model is not sensitive to the graph size. Our training graph has 80% of the edges, so naturally the raw degree counts will be higher than the testing graph.\n",
    "    indegrees = graph.in_degrees().float()\n",
    "    outdegrees = graph.out_degrees().float()\n",
    "\n",
    "    log_indegrees = torch.log(indegrees + 1)\n",
    "    log_outdegrees = torch.log(outdegrees + 1)\n",
    "\n",
    "    avg_log_indegree = log_indegrees.mean()\n",
    "    avg_log_outdegree = log_outdegrees.mean()\n",
    "\n",
    "    # zc and normalize\n",
    "    normalized_indegree = (log_indegrees - avg_log_indegree) / avg_log_indegree\n",
    "    normalized_outdegree = (log_outdegrees - avg_log_outdegree) / avg_log_outdegree\n",
    "\n",
    "    node_features = torch.stack([normalized_indegree, normalized_outdegree], dim=1)\n",
    "    graph.ndata['node_feats'] = node_features\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093e2fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 515080\n",
      "Number of edges: 4062676\n"
     ]
    }
   ],
   "source": [
    "training_graph = build_graph_from_df(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e98cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EDGE FEATS\n",
    "\n",
    "# # bookkeeping\n",
    "# training_graph.edata['transaction_id'] = torch.tensor(train_df['transaction_id'])\n",
    "\n",
    "# # numericals: we can include them directly\n",
    "# numericals = ['amount_received', 'amount_paid', 'hour_sin', 'hour_cos', 'time_normalized']\n",
    "# edge_numerical = torch.tensor(train_df[numericals].values, dtype=torch.float32)\n",
    "# training_graph.edata['numericals'] = edge_numerical\n",
    "\n",
    "# # categoricals: we use learned embeddings. This makes them transductive, but that is ok because currencies and payment types don't change often\n",
    "\n",
    "# payment_currency = torch.tensor(train_df['payment_currency_id'])\n",
    "# receiving_currency = torch.tensor(train_df['receiving_currency_id'])\n",
    "# payment_format = torch.tensor(train_df['payment_format_id'])\n",
    "# training_graph.edata['payment_currency'] = payment_currency\n",
    "# training_graph.edata['receiving_currency'] = receiving_currency\n",
    "# training_graph.edata['payment_format'] = payment_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NODE FEATS\n",
    "\n",
    "# # For each account, we compute their in and outdegrees\n",
    "# # We zero-center and log transform this value and normalize by the graph's average transformed in and outdegrees so that our model is not sensitive to the graph size. Our training graph has 80% of the edges, so naturally the raw degree counts will be higher than the testing graph.\n",
    "# indegrees = training_graph.in_degrees().float()\n",
    "# outdegrees = training_graph.out_degrees().float()\n",
    "\n",
    "# log_indegrees = torch.log(indegrees + 1)\n",
    "# log_outdegrees = torch.log(outdegrees + 1)\n",
    "\n",
    "# avg_log_indegree = log_indegrees.mean()\n",
    "# avg_log_outdegree = log_outdegrees.mean()\n",
    "\n",
    "# # zc and normalize\n",
    "# normalized_indegree = (log_indegrees - avg_log_indegree) / avg_log_indegree\n",
    "# normalized_outdegree = (log_outdegrees - avg_log_outdegree) / avg_log_outdegree\n",
    "\n",
    "# node_features = torch.stack([normalized_indegree, normalized_outdegree], dim=1)\n",
    "# training_graph.ndata['node_feats'] = node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Node Features (normalized degrees) =====\n",
      "\n",
      "----- Node Degrees (first 10 nodes) -----\n",
      "Node 0:\n",
      "  In-degree:  545  → normalized: +3.1889\n",
      "  Out-degree: 132597 → normalized: +7.6085\n",
      "Node 1:\n",
      "  In-degree:  328  → normalized: +2.8523\n",
      "  Out-degree: 80883 → normalized: +7.2477\n",
      "Node 2:\n",
      "  In-degree:  55  → normalized: +1.6754\n",
      "  Out-degree: 14711 → normalized: +6.0038\n",
      "Node 3:\n",
      "  In-degree:  53  → normalized: +1.6512\n",
      "  Out-degree: 10810 → normalized: +5.7789\n",
      "Node 4:\n",
      "  In-degree:  61  → normalized: +1.7430\n",
      "  Out-degree: 13641 → normalized: +5.9487\n",
      "\n",
      "===== Edge Features =====\n",
      "Numerical features shape: torch.Size([4062676, 5])\n",
      "Categorical feature 1 shape: torch.Size([4062676])\n",
      "Categorical feature 2 shape: torch.Size([4062676])\n",
      "Categorical feature 3 shape: torch.Size([4062676])\n",
      "\n",
      "===== Sample Edge Features =====\n",
      "Edge 0: 458857 -> 458857\n",
      "  Numerical: [-1.5293132066726685, -1.529313087463379, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 8, receiving currency: 8 , payment format: 5\n",
      "Edge 1: 236631 -> 236631\n",
      "  Numerical: [0.3639656603336334, 0.3639656603336334, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 5\n",
      "Edge 2: 244237 -> 229946\n",
      "  Numerical: [-0.9314213991165161, -0.9314213991165161, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 3, receiving currency: 3 , payment format: 4\n",
      "Edge 3: 20992 -> 20994\n",
      "  Numerical: [0.9639478325843811, 0.9639478325843811, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 6\n",
      "Edge 4: 19729 -> 19729\n",
      "  Numerical: [-1.5437047481536865, -1.5437047481536865, 0.0, 1.0, -1.377630352973938]\n",
      "  payment currency: 4, receiving currency: 4 , payment format: 5\n"
     ]
    }
   ],
   "source": [
    "# Check graph sanity\n",
    "\n",
    "# print(\"\\n===== Node Features (normalized degrees) =====\")\n",
    "# print(\"\\n----- Node Degrees (first 10 nodes) -----\")\n",
    "# indegrees = training_graph.in_degrees().float()\n",
    "# outdegrees = training_graph.out_degrees().float()\n",
    "\n",
    "# for i in range(5):\n",
    "#     norm_in = training_graph.ndata['node_feats'][i, 0].item()\n",
    "#     norm_out = training_graph.ndata['node_feats'][i, 1].item()\n",
    "#     print(f\"Node {i}:\")\n",
    "#     print(f\"  In-degree:  {indegrees[i].item():.0f}  → normalized: {norm_in:+.4f}\")\n",
    "#     print(f\"  Out-degree: {outdegrees[i].item():.0f} → normalized: {norm_out:+.4f}\")\n",
    "\n",
    "# print(\"\\n===== Edge Features =====\")\n",
    "# print(f\"Numerical features shape: {training_graph.edata['numericals'].shape}\")\n",
    "# print(f\"Categorical feature 1 shape: {training_graph.edata['payment_currency'].shape}\")\n",
    "# print(f\"Categorical feature 2 shape: {training_graph.edata['receiving_currency'].shape}\")\n",
    "# print(f\"Categorical feature 3 shape: {training_graph.edata['payment_format'].shape}\")\n",
    "\n",
    "# print(\"\\n===== Sample Edge Features =====\")\n",
    "# for i in range(5):\n",
    "#     src, dst = training_graph.edges()\n",
    "#     print(f\"Edge {i}: {src[i].item()} -> {dst[i].item()}\")\n",
    "#     print(f\"  Numerical: {training_graph.edata['numericals'][i].tolist()}\")\n",
    "#     print(f\"  payment currency: {training_graph.edata['payment_currency'][i].item()}, receiving currency: {training_graph.edata['receiving_currency'][i].item()} , payment format: {training_graph.edata['payment_format'][i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3610b26",
   "metadata": {},
   "source": [
    "# Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have both categorical and numerical edge features, we need a class to process and combine them\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_currencies=15, \n",
    "            num_payment_methods=7, \n",
    "            currencies_embed_dim=8, \n",
    "            payment_embed_dim=4, \n",
    "            num_numericals=5):\n",
    "        super().__init__()\n",
    "        self.currencies_embed = nn.Embedding(num_currencies, currencies_embed_dim)\n",
    "        self.payment_embed = nn.Embedding(num_payment_methods, payment_embed_dim)\n",
    "\n",
    "        # each edge has payment currency, receiving currency, payment method and numericals\n",
    "        self.out_dim = currencies_embed_dim + currencies_embed_dim + payment_embed_dim + num_numericals\n",
    "\n",
    "    def forward(self, payment_curr, receiving_curr, payment_method, numericals):\n",
    "        payment_curr_embed = self.currencies_embed(payment_curr)\n",
    "        receiving_curr_embed = self.currencies_embed(receiving_curr)\n",
    "        payment_method_embed = self.payment_embed(payment_method)\n",
    "\n",
    "        edge_feats = torch.cat([payment_curr_embed, receiving_curr_embed, payment_method_embed, numericals], dim=1)\n",
    "        return edge_feats\n",
    "\n",
    "# this returns our GNN-ready edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message passing layers\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, residual=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.residual = residual\n",
    "\n",
    "        if residual and input_dim != output_dim:\n",
    "            self.res_linear = nn.Linear(input_dim, output_dim) # project residual if diff dims\n",
    "        elif residual:\n",
    "            self.res_linear = nn.Identity()\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m': edges.src['Wh'] * edges.src['norm']} # normalize by source degree first\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        return {'h': torch.sum(nodes.mailbox['m'], dim=1)}\n",
    "    \n",
    "    def forward(self, graph, node_feats):\n",
    "        with graph.local_scope():\n",
    "            h_in = node_feats\n",
    "\n",
    "            graph.ndata['Wh'] = self.linear(node_feats) # W is linear transform, h is node feats\n",
    "\n",
    "            degs = graph.in_degrees().float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5).unsqueeze(1)\n",
    "            graph.ndata['norm'] = norm\n",
    "\n",
    "            graph.update_all(self.message_func, self.reduce_func)\n",
    "\n",
    "            h_out = graph.ndata['h'] * norm # then normalize by destination degree\n",
    "\n",
    "            h_out = F.relu(h_out)\n",
    "            if self.residual:\n",
    "                return h_out + self.res_linear(h_in)\n",
    "            else:\n",
    "                return h_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full classifier neural network\n",
    "\n",
    "class GCNEdgeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                # node params\n",
    "                node_in_feats=2, # log transformed and normalized indegree outdegree\n",
    "                hidden_dim=64,\n",
    "                num_gcn_layers=3,\n",
    "\n",
    "                # for EdgeEmbedding\n",
    "                num_currencies=15,\n",
    "                num_payment_methods=7,\n",
    "                currencies_embed_dim=8,\n",
    "                payment_embed_dim=4,\n",
    "                num_numericals=5,\n",
    "\n",
    "                # output classes\n",
    "                num_classes=2,\n",
    "\n",
    "                # regularization\n",
    "                dropout=0.2,\n",
    "                use_batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_gcn_layers = num_gcn_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(node_in_feats, hidden_dim, residual=False)) # no residual for layer 1 since in dim and out dim dont match, simpler to just skip\n",
    "        for i in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(hidden_dim, hidden_dim, residual=True))\n",
    "        if use_batch_norm: # create batch norm layers if using them\n",
    "            self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for i in range(num_gcn_layers)])\n",
    "\n",
    "        # Edge Feature processor\n",
    "        self.edge_embed = EdgeEmbedding(num_currencies, num_payment_methods, currencies_embed_dim, payment_embed_dim, num_numericals)\n",
    "\n",
    "        # Combiner to do edge classification: append 2 node embeddings with edge embedding\n",
    "        edge_repr_dim = 2*hidden_dim + self.edge_embed.out_dim\n",
    "\n",
    "        # MLP for edge classification\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(edge_repr_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2) if use_batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, graph, node_features, payment_curr, receiving_curr, payment_method, edge_numericals):\n",
    "\n",
    "        # GCN message passing to learn node embeddings\n",
    "        h = node_features\n",
    "        for i, gcn_layer in enumerate(self.gcn_layers):\n",
    "            h = gcn_layer(graph, h)\n",
    "            if self.use_batch_norm:\n",
    "                h = self.batch_norms[i](h)\n",
    "            if i < self.num_gcn_layers - 1: # dropout between each layer except after last\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # get node embedding for each edge\n",
    "        src_nodes, dst_nodes = graph.edges()\n",
    "        src_embed = h[src_nodes]\n",
    "        dst_embed = h[dst_nodes]\n",
    "\n",
    "        # process edge features\n",
    "        edge_features = self.edge_embed(payment_curr, receiving_curr, payment_method, edge_numericals)\n",
    "\n",
    "        # concat 2 nodes embeddings with edge features to get complete edge embedding\n",
    "        edge_repr = torch.cat([src_embed, dst_embed, edge_features], dim=1)\n",
    "\n",
    "        # classify edges\n",
    "        logits = self.edge_classifier(edge_repr)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94602494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define focal CE loss since we have extreme class imbalance\n",
    "\n",
    "def compute_class_weights(labels): \n",
    "    class_counts = torch.bincount(labels)\n",
    "    total = len(labels)\n",
    "    num_classes = len(class_counts)\n",
    "    weights = total / (num_classes * class_counts.float())\n",
    "    return weights # to pass into focal loss as alpha\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.shape[1])\n",
    "        p_t = (probs * targets_one_hot).sum(dim=1) # probability of true class in dataset\n",
    "        focal_weight = (1 - p_t) ** self.gamma # easy examples p_t near 1, focal weight near 0\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]  # [N]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean() #default\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf43776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, split_mask=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \n",
    "    Args:\n",
    "        split_mask: Boolean mask for edges to evaluate (None = all edges)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(g, g.ndata['feat'], g.edata['cat1'], \n",
    "                      g.edata['cat2'], g.edata['numerical'])\n",
    "        \n",
    "        if split_mask is not None:\n",
    "            logits = logits[split_mask]\n",
    "            labels = g.edata['label'][split_mask]\n",
    "        else:\n",
    "            labels = g.edata['label']\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # Probability of positive class\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = (preds == labels).float().mean().item()\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            labels.cpu().numpy(), \n",
    "            preds.cpu().numpy(),\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # AUC (if both classes present)\n",
    "        try:\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), probs.cpu().numpy())\n",
    "        except:\n",
    "            auc = 0.0\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': support,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        return metrics, preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42163b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ced3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup model and loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
